---
title: "Final_Project"
author: "Cameron Chambers, Bram Stults, Priscilla Wen"
date: "2024-04-02"
output: html_document
#output: beamer_presentation
header-includes:
   - \usepackage{multirow}
editor_options: 
  chunk_output_type: inline
urlcolor: blue
---

# Introduction

Modeling with *unsupervised learning* forgoes the precise prediction of a measured outcome in order to describe patterns of association among 'input measures': the many observations of a feature space (page *xi*, Hastie et al., 2009). We have only $N$-observations $(x_1, x_2, ... x_N)$ of a random $p$-vector $X$ and we wish to directly infer properties of the joint-probability distribution $Pr(X)$. "Cluster analysis attempts to find multiple convex regions of the X-space that contain modes of Pr(X)" (pg. *xi*, Hastie et al., 2009). In essence, we are attempting to see whether $Pr(X)$ can be represented by simpler densities which represent distinct types of constituent observations of $X$.

Hierarchical clustering does not require the titular, initializing '$k$' of the $K$-means algorithm, nor an initializing *guess* as to centroids of clusters. Rather, hierarchical clustering is based on a chosen measure of *dissimilarity* among pairwise observations. These pairwise 'dissimilarities' are the basis for the disjunction of groups of observations: clusters. This, in its turn, is a modular description of one part of an emergent hierarchy. The totality of data as a single cluster composed of $n$-objects is gradually split at each lower level into smaller clusters, with each cluster's internal dissimilarity being reduced. The final hierarchy establishes each individual observation isolated in its own 'cluster' at the lowest level, entirely 'similar' to itself. This 'divisive' hierarchy then is formed in $n-1$ steps with $2^{n-1}-1$ possibilities to split the data into two clusters.

## Background

Divisive or 'top-down' clustering algorithms have, by the past, been studied less extensively than bottom-up or agglomerative clustering (pg. 486, Hastie et al., 2009). Yet a major forthcoming advantage of divisive hierarchical methods is that they may be parametrized to partition the data into a restricted number of clusters. This is commonly known by its moniker 'DIANA' in the field data analytics.

# Mathematics behind Divisive Hierarchical Clustering

TO-DO:

Cover distance metric formulas for:\
"euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski"

Introduce object known as distance or proximity matrix (show example)

## **DIANA in R**

The `cluster` library contains the function `diana()`:

```{r function_intro, echo=T}
diana(x, diss = inherits(x, "dist"), metric = "euclidean", stand = FALSE,
      stop.at.k = FALSE,
      keep.diss = n < 100, keep.data = !diss, trace.lev = 0)
```

**x**: the main argument, representing the data matrix or distance matrix used for clustering.

**diss**: A logical value indicating if x already contains a distance matrix. Defaults to TRUE if x inherits from the "dist" class.

**metric**: The distance metric used for calculating distances between data points. Defaults to "euclidean" (Euclidean distance).

**stand**: A logical value indicating whether to standardize the data before clustering. Defaults to FALSE.

**stop.at.k**: A logical value indicating whether to stop clustering at a specific number of clusters (k). Defaults to FALSE (continues until all data points are in separate clusters). **NOT YET IMPLEMENTED (Instead** we would use something like: `cut_clusters <- cutree( diana_result, k=4)` )

## Let's try an example!

A data set containing a rubric of information about the films of Studio Ghibli... this studio famously deploys little-to-no marketing campaigns in the distribution of its films, relying on the excitement and interest of audiences.

```{r dataset_intro, message=FALSE, eval=T, echo=T}
library(dendextend)
library(ggplot2)
library(plotly)
library(factoextra)
library(cluster)
library(stats)
library(kableExtra)

# Read the Studio Ghibli dataset from URL
url <- "https://raw.githubusercontent.com/cham24/Math_3190_Final_Project/main/Studio%20Ghibli%20%5Bwith%20rating%5D.csv"
data <- read.csv(url, header = TRUE)

# Remove rows with missing values and select specific columns
data_new <- na.omit(data)
data_new <- data_new[c("Year", "Budget", "Revenue", "Duration", "Rating.based.on.10.", "Vote..in.kilo.")]

# Rename columns
col_names <- c("Year", "Budget_in_$M", "Revenue_in_$", "Duration_in_Minutes", "Rating", "Vote_in_kilo")
colnames(data_new) <- col_names

```

Here we see the very basic example using classic Euclidean distance:

```{r basic_euclidean_diana, eval=True, echo=True}

# diana() clustering
  
hc_euclid <- diana(
    x = dist(data_new, method = 'euclidean'),     # x is a distance matrix
    diss = TRUE,                                  # dist() creates a distance matrix
    keep.diss = nrow(data_new) < 100,             # Control distance matrix storage (optional)
    keep.data = FALSE,                            # Avoid keeping original data (optional)
    stop.at.k = FALSE                             # non-default is not yet implemented!
  )
  
  print(hc_euclid)
  
```

A basic dendrogram and banner plot can be directly plotted using plot() with the cluster object:

```{r basic_plot_1, eval=T, echo=T}
  plot(hc_euclid)
```

Banner plots display the hierarchy of clusters, equivalent to a tree (Rousseeuw (1986) or chapter 6 of Kaufman and Rousseeuw (1990)) where banners plot the diameter of each cluster being split.

```{r}
cut_clusters <- cutree(hc_euclid, k=4)
print(cut_clusters)
```

```{r}
# List of distance metrics
distance_metrics <- c("euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski")

#  divisive hierarchical clustering for each distance metric
diana_results <- lapply(distance_metrics, function(distance_metric) {
  
  # diana() clustering
  hc <- diana(
    x = dist(data_new, method = distance_metric), # x is a distance matrix
    diss = TRUE,                                  # dist() creates a distance matrix
    keep.diss = nrow(data_new) < 100,             # Control distance matrix storage (optional)
    keep.data = FALSE                             # Avoid keeping original data (optional)
  )
  
  return(hc)
})

# diana_results will be a list containing clustering objects (hc) for each distance metric
```

# **Dendrogram**

\
The most common diagnostic measure in hierarchical clustering is arguably the dendrogram visualization. Dendrograms provide a graphical representation of the clustering process, showing how clusters merge or split at each step. They are widely used because they offer intuitive insights into the hierarchical structure of the data and help in determining the optimal number of clusters.

## RShiny App : Reactive Dendrogram

```{r shiny, echo = T, eval = F, message = F}
library(shiny)
library(shinydashboard)
library(stats)

ui <- dashboardPage(
  dashboardHeader(title = "Dendrogram Viewer for Studio Ghibli Dataset"),
  dashboardSidebar(
    fileInput("dataFile", "Upload your Studio Ghibli dataset", accept = ".csv"),
    selectInput("numColumns", "Select numeric columns for clustering",
                choices = NULL, selected = NULL, multiple = TRUE),
    selectInput("distanceType", "Select Distance Type",
                choices = c("euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski"), selected = "euclidean"),
    selectInput("linkageMethod", "Select Linkage Method",
                choices = c("ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid"), selected = "complete"),
    actionButton("update", "Update Selection")
  ),
  dashboardBody(
    plotOutput("dendrogramPlot"),
    verbatimTextOutput("fileInfo")
  )
)

server <- function(input, output, session) {
  data <- reactive({
    inFile <- input$dataFile
    if (is.null(inFile)) {
      return(NULL)
    }
    read.csv(inFile$datapath, na.strings = c("NA", "na", ""))
  })
  
  observe({
    df <- data()
    updateSelectInput(session, "numColumns", choices = names(df)[sapply(df, is.numeric)])
  })

  output$fileInfo <- renderPrint({
    df <- data()
    if (is.null(df)) {
      "No file uploaded."
    } else {
      summary(df)
    }
  })

  output$dendrogramPlot <- renderPlot({
    req(input$update)
    df <- data()
    if (is.null(df) || is.null(input$numColumns)) {
      return()
    }
    clean_data <- df[, input$numColumns]
    clean_data <- apply(clean_data, 2, function(column) {
      if (any(is.na(column))) {
        column[is.na(column)] <- mean(column, na.rm = TRUE)
      }
      return(column)
    })

    dist_matrix <- tryCatch({                      
      dist(clean_data, method = input$distanceType) #dist() method
    }, warning = function(w) {
      cat("Warning: ", w$message, "\n")
      return(NULL)
    }, error = function(e) {
      cat("Error: ", e$message, "\n")
      return(NULL)
    })

    if (!is.null(dist_matrix)) {
      hc <- hclust(dist_matrix, method = input$linkageMethod) 
      #this is specifically for agglomerative
      plot(hc, main = "Dendrogram of Agglomerative Hierarchical Clustering")
    }
  })
}

shinyApp(ui, server)
```

```{r shiny2, echo = T, eval = F, message = F}
library(dendextend)
library(ggplot2)
library(plotly)
library(shiny)
library(factoextra)
library(cluster)
library(stats)

data = read.csv("Studio Ghibli [TEST].csv", header = TRUE, row.names = 1)

data_new <- na.omit(data)
data_new <- data_new[c("Year", "Budget", "Revenue", "Duration")]
col_names <- c("Year", "Budget_in_$M", "Revenue_in_$", "Duration_in_Minutes")
data_new <- setNames(data_new, col_names)
# data_new <- matrix(data_new)

data_new

clean_data <- apply(data_new, 2, function(column) {
  if(any(is.na(column))) {
    column[is.na(column)] <- mean(column, na.rm = TRUE)
  }
  return(column)
})

dend <- data_new |> 
  dist() |> 
  hclust(method = "ave") |>  # "ave" is default: the average linkage method
  as.dendrogram()
dend2 <- color_branches(dend, 5)

p <- ggplot(dend2, horiz = T, offset_labels = -3)
ggplotly(p)



# data_new
# write.csv(data_new, "Studio Ghiblie [TEST2].csv")
```

# Diagnostic and Remedial Measures

## I. Cophenetic Distance & Correlation

TO-DO : give explanation/definition of cophenetic distance

***Cophenetic correlation*** is a statistic representing how well a dendrogram preserves the pairwise distances between data points. It assesses the fidelity of a hierarchical clustering solution by comparing the distances in the original data space with the distances represented by the dendrogram. A high cophenetic correlation indicates that the dendrogram accurately represents the underlying pairwise distances between data points.

**(1) Compute the Original Pairwise Distances**: Start by calculating the pairwise distances between all data points in the original dataset using an appropriate distance metric (e.g., Euclidean distance, Manhattan distance, etc.).

**(2) Compute Cophenetic Distances**: For each pair of data points, calculate the cophenetic distance, which is the height at which the two points are first merged into the same cluster in the dendrogram.

**(3) Compute Cophenetic Correlation**: Once you have the original pairwise distances and the corresponding cophenetic distances, calculate the correlation coefficient between these two sets of distances. This correlation coefficient is the cophenetic correlation.

Mathematically, the cophenetic correlation $c$ is computed using the following formula:

$$c = \frac{\sum_{i < j}(d_{ij} - \bar{d})(c_{ij} - \bar{c})}{\sqrt{\sum_{i < j}(d_{ij} - \bar{d})^2 \sum_{i < j}(c_{ij} - \bar{c})^2}}$$

Where:

-   $d_{ij}$ is the original pairwise distance between data points $i$ and $j$.

-   $c_{ij}$ is the cophenetic distance between data points $i$ and $j$ in the dendrogram.

-   $\bar{d}$ is the mean of the original pairwise distances.

-   $\bar{c}$ is the mean of the cophenetic distances.

-   The summations are over all pairs of data points $i$ and $j$ with $i$ \< $j$.

The resulting cophenetic correlation coefficient $c$ ranges from -1 to 1, where a value close to 1 indicates high similarity between the original pairwise distances and the distances represented by the dendrogram.

```{r cophenetic_cor, echo = T, eval = F, warning=F, message = F}


# Perform divisive hierarchical clustering and calculate cophenetic metrics
diana_results <- lapply(distance_metrics, function(distance_metric) {
  
  # Compute distance matrix
  distance_matrix <- dist(data_new, method = distance_metric)
  
  # Perform diana() clustering
  hc <- diana(
    x = distance_matrix,
    diss = TRUE,
    keep.diss = nrow(data_new) < 100,
    keep.data = FALSE
  )
  
  # Calculate cophenetic distance and correlation
  cophenetic_distance <- cophenetic(hc)
  cophenetic_correlation <- cor(cophenetic_distance, distance_matrix)
  
  # Return a list with clustering object, cophenetic distance, and correlation
  return(list(
    hc = hc,
    cophenetic_distance = cophenetic_distance,
    cophenetic_correlation = cophenetic_correlation
  ))
})

# Find the result with the highest cophenetic correlation
best_index <- which.max( sapply(diana_results, function(result) result$cophenetic_correlation))
best_result <- diana_results[[best_index]]
best_distance_metric <- distance_metrics[best_index]

# Print results (optional)
cat("Distance metric with highest cophenetic correlation:", best_distance_metric, "\n")
cat("Cophenetic correlation:", best_result$cophenetic_correlation, "\n")
```

Highest Cophenetic correlation is 0.9986 , which is obtained with MAXIMUM distance.

This correlation value indicates a strong relationship between the original pairwise distances and the cophenetic distances present in the dendrogrammatic representation.

```{r dendrogram1, echo = T, eval = F, warning=F, message = F}
# Perform divisive hierarchical clustering
best_hc <- diana(dist(data_new, method = "maximum"))

# Color branches of the dendrogram
best_hc_colored <- color_branches(as.dendrogram(best_hc), k = 5)  # You can adjust the number of colors (k) as needed

# Plot dendrogram
plot(best_hc_colored, main = "Dendrogram with MAXIMUM distance")
```

**How to use dendrogram?**

**(1) Visual Inspection:**

-   **Observe the Overall Structure**: Start by examining the overall structure of the dendrogram. Look for patterns of clustering and the hierarchy of clusters.

-   **Check the Branch Lengths**: Longer branches represent greater dissimilarity between clusters. Pay attention to abrupt changes in branch lengths, which may indicate significant differences between clusters.

-   **Identify Cluster Cuts**: Look for natural breaks or clusters where the dendrogram is cut. These cuts represent the division points where clusters are formed.

**(2) Determining the Number of Clusters:**

-   **Horizontal Cuts**: Imagine drawing a horizontal line across the dendrogram. The number of times this line intersects the vertical lines (branches) indicates the number of clusters.

-   **Natural Breaks**: Identify points in the dendrogram where there are large gaps between branches. These points often correspond to natural breaks in the data and suggest the optimal number of clusters.

**(3) Assessing Cluster Sizes:**

-   **Cluster Size Distribution**: Examine the distribution of cluster sizes by looking at the heights of the dendrogram branches. Ensure that the resulting clusters are of reasonable sizes and not overly skewed.

-   **Balanced Clusters**: Ideally, clusters should have approximately equal numbers of data points, but this may vary depending on the nature of the data and the clustering objectives.

**(4) Identifying Inconsistencies:**

-   **Inconsistent Merging**: Look for instances where clusters are merged at varying distances. Inconsistent merging may indicate suboptimal clustering or noisy data.

-   **Abnormal Patterns**: Watch out for abnormal patterns such as clusters merging and then splitting again shortly afterward. These patterns may suggest errors in the clustering process or the presence of outliers.

**(5) Iterative Refinement:**

-   **Experiment with Parameters**: Adjust clustering parameters such as distance metrics or linkage methods and observe the changes in the dendrogram structure.

-   **Compare Results**: Compare dendrograms generated with different parameter settings to identify the most stable and meaningful clustering solution.

```{r dendrogram2, echo = T, eval = F, warning=F, message = F}
# Perform hierarchical clustering with the chosen parameters
best_hc <- diana(dist(data_new, method = "maximum"))

# Cut the dendrogram to get the clusters
clusters <- cutree(best_hc, k = 3)  
# You can adjust the number of clusters as needed, here we use 3

# Plot the dendrogram with cluster colors
plot(as.dendrogram(best_hc), main = "Dendrogram with MAXIMUM distance and average linkage (Colored)",
     xlab = "Observations", ylab = "Height")
rect.hclust(best_hc, k = 3, border = 2:4)  # Highlight clusters
```

```{r cluster_profiles, echo = T, eval = F, warning=F, message = F}

# Cut the dendrogram to get a restricted number clusters
clusters <- cutree(best_hc, k = 3)  # Using 3 clusters

# Combine the original dataframe with the cluster labels
data_clusters <- cbind(data_new, cluster = clusters)
data_clusters

# Count the number of observations in each cluster
cluster_counts <- table(clusters)

# Print the number of observations in each cluster
print("Number of observations in each cluster:")
print(cluster_counts)

# Cluster profiling: calculate summary statistics for each cluster
cluster_profiles <- aggregate(data_clusters[, -7], by = list(cluster = clusters), FUN = mean)
cluster_profiles
```

3 Clusters:

-   Cluster 1 contains 14 observations: 1, 12, 20, 14, 4, 21, 8, 2, 7, 9, 13, 17, 18, 22

-   Cluster 2 contains 8 observations: 3, 5, 11, 23, 15, 6, 10, 16

-   Cluster 3 contains 1 observation: 19

#### Removing outlier

It appears that observation 19 is an outlier as it is the only observation in Cluster 3. From the dendrogram this point significantly differs from the rest of the data, either due to measurement errors or for some other reasons. Noting this, we remove this outlier for further analysis.

```{r data_no_outlier, echo = T, eval = F, warning=F, message = F}
# Remove outlier observation (19)
data_no_outlier <- data_new[-19, ]
# Print the dimensions of the new dataframe
print(dim(data_no_outlier))
```

```{r cophenetic_cor_no_outlier, echo = T, eval = F, warning=F, message = F}
# Perform divisive hierarchical clustering and calculate cophenetic metrics

diana_results <- lapply(distance_metrics, function(distance_metric) {
  
  # Compute distance matrix
  distance_matrix2 <- dist(data_no_outlier, method = distance_metric)
  
  # Perform diana() clustering
  hc2 <- diana(
    x = distance_matrix2,
    diss = TRUE,
    keep.diss = nrow(data_no_outlier) < 100,
    keep.data = FALSE
  )
  
  # Calculate cophenetic distance and correlation
  cophenetic_distance2 <- cophenetic(hc2)
  cophenetic_correlation2 <- cor(cophenetic_distance2, distance_matrix2)
  
  # Return a list with clustering object, cophenetic distance, and correlation
  return(list(
    hc = hc2,
    cophenetic_distance = cophenetic_distance2,
    cophenetic_correlation = cophenetic_correlation2
  ))
})

# Find the result with the highest cophenetic correlation
best_index2 <- which.max(sapply(diana_results, function(result) result$cophenetic_correlation))

best_result2 <- diana_results[[best_index2]]

best_distance_metric2 <- distance_metrics[best_index2]

# Print results (optional)
cat("Distance metric with highest cophenetic correlation:", best_distance_metric2, "\n")
cat("Cophenetic correlation:", best_result2$cophenetic_correlation, "\n")
```

```{r dendrogram_no_outlier, echo = T, eval = F, warning=F, message = F}
# Perform hierarchical clustering with the chosen parameters
best_hc2 <- diana(dist(data_no_outlier, method = "maximum"))

# Cut the dendrogram to get the clusters
clusters2 <- cutree(best_hc2, k = 3)  # You can adjust the number of clusters as needed

# Plot the dendrogram with cluster colors
plot(as.dendrogram(best_hc2), main = "Dendrogram with MAXIMUM distance (without outlier)",
     xlab = "Observations", ylab = "Height")
rect.hclust(best_hc2, k = 3, border = 2:8)  # Highlight clusters
```

```{r cluster_profiles2, echo = T, eval = F, warning=F, message = F}
# Perform hierarchical clustering with the chosen parameters
best_hc2 <- diana(dist(data_no_outlier, method = "maximum"))

# Cut the dendrogram to get the clusters
clusters2 <- cutree(best_hc2, k = 3)  # Set the number of clusters to 3

# Combine the original dataframe with the cluster labels
data_clusters2 <- cbind(data_no_outlier, cluster = clusters2)
data_clusters2
# Count the number of observations in each cluster
cluster_counts2 <- table(clusters2)

# Print the number of observations in each cluster
print("Number of observations in each cluster:")
print(cluster_counts2)

# Cluster profiling: calculate summary statistics for each cluster
cluster_profiles2 <- aggregate(data_clusters2[, -7], by = list(cluster = clusters2), FUN = mean)
cluster_profiles2
```

3 Clusters (without outlier):

-   Cluster 1 contains 14 observations: 1, 12, 20, 14, 4, 21, 8, 2, 7, 9, 13, 17, 18, 22

-   Cluster 2 contains 8 observations: 3, 5, 11, 23, 15, 6

-   Cluster 3 contains 1 observation: 10, 16

## **II. Silhouette Coefficient**

The Silhouette Coefficient is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). Silhouette Coefficient measures the relative distance between a data point and its own cluster versus neighboring clusters, providing insight into the quality and separation of clusters in a dataset.

It ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.

How to Compute Silhouette Coefficient?

(1) Compute the average distance from a single data point $i$ to all other points in the same cluster. Let this be denoted as $a_i$​.

(2) For each neighboring cluster (clusters other than the cluster $i$ belongs to), compute the average distance from $i$ to all points in that cluster. Let the minimum of these distances be denoted as $b_i$​.

(3) Compute the silhouette coefficient $s_i$​ for data point $i$ using the formula:

$$
s_i=\frac{b_i - a_i}{max(a_i, b_i)}
$$

(4) Repeat steps (1)-(3) for all data points.

(5) Finally, compute the overall Silhouette Coefficient as the mean of all $s_i$​ values.

$s_i​$ represents the Silhouette Coefficient for data point $i$, $a_i$​ represents the average distance from $i$ to all other points in the same cluster, and $b_i$​ represents the minimum average distance from $i$ to all points in neighboring clusters.

```{r silhouette, warning=F}
library(cluster)

# Compute silhouette scores
sil_scores <- silhouette(clusters2, dist(data_clusters2))
sil_scores
```

```{r silhouette_graph, warning=F}
library(cluster)
library(factoextra)

# Fit hierarchical clustering with the chosen parameters
best_hc2 <- diana(dist(data_no_outlier, method = "maximum"))
clusters2 <- cutree(best_hc2, k = 3)  # Set the number of clusters to 3

# Create a silhouette plot
silhouette_plot <- fviz_silhouette(
  silhouette(clusters2, dist(data_clusters2[, -ncol(data_clusters2)])),
  geom="bar",
  main="Silhouette Plot for Clusters"
)

# Show the silhouette plot
print(silhouette_plot)
```

The average silhouette width for each cluster provides insights into the quality and separation of the clusters. Here's an observation for each cluster based on the average silhouette width:

**(1) Cluster 1 (14 observations, average silhouette width: 0.78):**

-   This cluster exhibits a high average silhouette width, indicating that most observations within this cluster are well-suited to their assigned cluster and less suited to neighboring clusters.

-   The elevated average silhouette width implies strong separation and cohesion within the cluster, with distinct boundaries between clusters.

**(2) Cluster 2 (6 observations, average silhouette width: 0.59):**

-   This cluster demonstrates a moderate average silhouette width, suggesting that observations within this cluster are reasonably well-matched to their own cluster but may also share similarities with neighboring clusters.

-   The moderate average silhouette width indicates moderate separation and cohesion within the cluster, with some instances of overlap or ambiguity.

**(3) Cluster 3 (2 observation, average silhouette width: 0.56):**

-   This cluster comprises only two observations, and despite its small size, it still manages to achieve a positive average silhouette width of 0.56.

-   This suggests that these two observations are more similar to each other than they are to observations in other clusters.

# Completing Analysis

```{r cluster description, warning=FALSE}
cluster_profiles2
```

## Cluster Description

Let's summarize the features of each cluster based on the provided cluster profiles:

**Cluster 1:**

-   **Year:** Films in this cluster tend to be from around 1996.57.

-   **Budget:** The average budget for films in this cluster is approximately \$93.24 million.

-   **Revenue:** Films in this cluster typically generate revenue of around \$26,222,059.

-   **Duration:** The average duration of films in this cluster is approximately 102.93 minutes.

-   **Rating:** Films in this cluster have an average rating of about 7.61.

-   **Vote Count:** The average number of votes for films in this cluster is around 122.21 thousand.

**Cluster 2:**

-   **Year:** Films in this cluster are more recent, with an average release year of around 2008.33.

-   **Budget:** The average budget for films in this cluster is lower than Cluster 1, at about \$38.50 million.

-   **Revenue:** Films in this cluster tend to have higher revenue, averaging around \$162,136,149.

-   **Duration:** The average duration of films in this cluster is approximately 113.67 minutes.

-   **Rating:** Films in this cluster have a slightly higher average rating of about 7.67.

-   **Vote Count:** The average number of votes for films in this cluster is around 144.00 thousand.

**Cluster 3:**

-   **Year:** Films in this cluster are from around 2002.50 on average.

-   **Budget:** The average budget for films in this cluster is the lowest among the clusters, at approximately \$21.50 million.

-   **Revenue:** Films in this cluster tend to have the highest revenue, averaging around \$255,487,426.

-   **Duration:** The average duration of films in this cluster is the longest, at about 122.00 minutes.

-   **Rating:** Films in this cluster have the highest average rating of about 8.40.

-   **Vote Count:** Films in this cluster also have the highest average vote count, at around 647.50 thousand.

# Strengths, Weaknesses

### Strengths:

1.  **Interpretability**: Divisive clustering produces a hierarchical structure of clusters, making it easier to interpret and understand the relationships between clusters at different levels of granularity. This hierarchical representation can provide insights into the natural grouping of data and the hierarchical organization of clusters.

2.  **Flexibility**: Divisive clustering allows for the exploration of clusters at multiple levels of detail. Analysts can start with a single cluster encompassing all data points and recursively divide it into smaller clusters until reaching the desired level of granularity. This flexibility allows for adaptive clustering based on the characteristics of the data and the analytical objectives.

3.  **Scalability**: Divisive clustering can be more computationally efficient than agglomerative clustering, especially for large datasets. Since divisive clustering starts with a single cluster and recursively divides it into smaller clusters, it can be less computationally intensive than agglomerative clustering, which involves pairwise comparisons and merging of clusters.

4.  **Hierarchy Preservation**: Divisive clustering preserves the hierarchical relationships between clusters throughout the clustering process. This hierarchical structure provides additional context and information about the relationships between clusters, which can be useful for downstream analysis tasks such as visualization, interpretation, and pattern discovery.

5.  **Top-Down Exploration**: Divisive clustering allows analysts to perform top-down exploration of the data, starting with a global view of the entire dataset and progressively refining the clustering structure to reveal finer-grained patterns and relationships. This top-down approach can be beneficial for gaining insights into the hierarchical organization of data and identifying clusters at different levels of abstraction.

### Weaknesses:

1.  **Sensitivity to Initial Conditions**: Divisive clustering heavily relies on the initial selection of clusters, which can affect the final clustering results. Depending on the choice of initial cluster seeds, the algorithm may converge to different solutions, leading to variations in the clustering outcome.

2.  **Computational Complexity**: Divisive clustering can be computationally intensive, especially for large datasets. Since it involves recursively dividing clusters into smaller clusters, the algorithm's complexity increases with the number of data points and the depth of the clustering hierarchy. This can make divisive clustering less scalable for very large datasets compared to other clustering methods.

3.  **Dependency on Distance Metric**: The choice of distance metric used in divisive clustering can significantly impact the clustering results. Different distance metrics may lead to different clustering outcomes, making it important to carefully select an appropriate distance measure based on the characteristics of the data and the clustering objectives.

# References

-   Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Second Edition. Springer, 2009.

-   mitran-lab.amath.nuc.edu/courses/MATH590/biblio/Clustering.ch7.HierarchicalClustering.pdf, SIAM Online Journals \<\<\<\<\<\<\< HEAD

-   A. Kassambara, Practical Guide to Cluster Analysis in R , ch9, page 84-96,

-   Everitt, B. S., Landau, S., Leese, M., & Stahl, D. (2011). Hierarchical clustering. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 1(1), 14-23.

-   Murtagh, F. (2011). On ultrametricity, data coding, and computation. Journal of Classification, 28(3), 326-347.

-   R Development Core Team (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing.

-   Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20, 53-65.

-   Tibshirani, R., Walther, G., & Hastie, T. (2001). Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), 411-423.
