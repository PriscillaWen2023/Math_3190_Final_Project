---
title: "Final_Project"
author: "Cameron Chambers, Bram Stults, Priscilla Wen"
date: "2024-04-02"
output: html_document
header-includes:
   - \usepackage{multirow}
editor_options: 
  chunk_output_type: console
urlcolor: blue
---

# Introduction

Unsupervised learning is modeling practice which forgoes the precise prediction of a measured outcome in order to describe patterns of association among 'input measures': the many observations of a feature space (page *xi*, Hastie et al., 2009). We have only $N$-observations $(x_1, x_2, ... x_N)$ of a random $p$-vector $X$ and we wish to directly infer properties of the joint-probability distribution $Pr(X)$. "Cluster analysis attempts to find multiple convex regions of the X-space that contain modes of Pr(X)" (pg. *xi*, Hastie et al., 2009). In essence, we are attempting to see whether $Pr(X)$ can be represented by simpler densities which represent distinct types of constituent observations of $X$.

<<<<<<< HEAD
Hierarchical clustering does not require the titular, initializing '$k$' of the $K$-means algorithm, nor an initializing *guess* as to centroids of clusters. Rather, hierarchical clustering is based on a chosen measure of *dissimilarity* among pairwise observations. These pairwise 'dissimilarities' are the basis for the disjunction of groups of observations: clusters. This, in its turn, is a modular description of one part of an emergent hierarchy. The totality of data as a single cluster composed of $n$-objects is gradually split at each lower level into smaller clusters, with each cluster's internal dissimilarity being reduced. The final hierarchy establishes each individual observation isolated in its own 'cluster' at the lowest level, entirely 'similar' to itself. This 'divisive' hierarchy then is formed in $n-1$ steps with $2^{n-1}-1$ possibilities to split the data into two clusters.

Divisive or 'top-down' clustering algorithms have, by the past, been studied less extensively than bottom-up or agglomerative clustering (pg. 486, Hastie et al., 2009). Yet a major forthcoming advantage of divisive hierarchical methods is that they may be parametrized to partition the data into a restricted number of clusters. This is commonly known by its moniker 'DIANA' in the field data analytics.
=======
Hierarchical clustering does not require the titular, initializing '$k$' of the $K$-means algorithm, nor an initializing *guess* as to centroids of clusters.  Rather, hierarchical clustering is based on a chosen measure of _dissimilarity_ among pairwise observations.  These pairwise 'dissimilarities' are the basis for the disjunction of groups of observations: clusters.  This, in its turn, is a modular description of one part of an emergent hierarchy.  The totality of data as a single cluster composed of $n$-objects is gradually split at each lower level into smaller clusters, with each cluster's internal dissimilarity being reduced. The final hierarchy establishes each individual observation isolated in its own 'cluster' at the lowest level, entirely 'similar' to itself.  This 'divisive' hierarchy then is formed in $n-1$ steps with $2^{n-1}-1$ possibilities to split the data into two clusters.

Divisive or 'top-down' clustering algorithms have, by the past, been studied less extensively than bottom-up or agglomerative clustering (pg. 486, Hastie et al., 2009). Yet a major forthcoming advantage of divisive hierarchical methods is that they may be parametrized to partition the data into a restricted number of clusters. This is commonly known by its moniker 'DIANA' in the field data analytics.

>>>>>>> 0d067522c594f10d0e00f2b4c27164c55686e106

### Background

//Discuss background of our project here

# Mathematics behind Divisive Hierarchical Clustering

# Analysis on our Dataset

//Analysis here

# Diagnostic and Remedial Measures

### **1. Dendrogram:**

\
The most common diagnostic measure in hierarchical clustering is arguably the dendrogram visualization. Dendrograms provide a graphical representation of the clustering process, showing how clusters merge or split at each step. They are widely used because they offer intuitive insights into the hierarchical structure of the data and help in determining the optimal number of clusters.

**How to use dendrogram?**

**(1) Visual Inspection:**

-   **Observe the Overall Structure**: Start by examining the overall structure of the dendrogram. Look for patterns of clustering and the hierarchy of clusters.

-   **Check the Branch Lengths**: Longer branches represent greater dissimilarity between clusters. Pay attention to abrupt changes in branch lengths, which may indicate significant differences between clusters.

-   **Identify Cluster Cuts**: Look for natural breaks or clusters where the dendrogram is cut. These cuts represent the division points where clusters are formed.

**(2) Determining the Number of Clusters:**

-   **Horizontal Cuts**: Imagine drawing a horizontal line across the dendrogram. The number of times this line intersects the vertical lines (branches) indicates the number of clusters.

-   **Natural Breaks**: Identify points in the dendrogram where there are large gaps between branches. These points often correspond to natural breaks in the data and suggest the optimal number of clusters.

**(3) Assessing Cluster Sizes:**

-   **Cluster Size Distribution**: Examine the distribution of cluster sizes by looking at the heights of the dendrogram branches. Ensure that the resulting clusters are of reasonable sizes and not overly skewed.

-   **Balanced Clusters**: Ideally, clusters should have approximately equal numbers of data points, but this may vary depending on the nature of the data and the clustering objectives.

**(4) Identifying Inconsistencies:**

-   **Inconsistent Merging**: Look for instances where clusters are merged at varying distances. Inconsistent merging may indicate suboptimal clustering or noisy data.

-   **Abnormal Patterns**: Watch out for abnormal patterns such as clusters merging and then splitting again shortly afterward. These patterns may suggest errors in the clustering process or the presence of outliers.

**(5) Iterative Refinement:**

-   **Experiment with Parameters**: Adjust clustering parameters such as distance metrics or linkage methods and observe the changes in the dendrogram structure.

-   **Compare Results**: Compare dendrograms generated with different parameter settings to identify the most stable and meaningful clustering solution.

**How to compute cophenetic correlation?**

Cophenetic correlation is a measure of how well a dendrogram preserves the pairwise distances between data points. It assesses the fidelity of a hierarchical clustering solution by comparing the distances in the original data space with the distances represented by the dendrogram. A high cophenetic correlation indicates that the dendrogram accurately represents the underlying pairwise distances between data points.

**(1) Compute the Original Pairwise Distances**: Start by calculating the pairwise distances between all data points in the original dataset using an appropriate distance metric (e.g., Euclidean distance, Manhattan distance, etc.).

**(2) Perform Hierarchical Clustering**: Apply hierarchical clustering to the dataset using a chosen linkage method and distance metric. This results in a dendrogram representing the hierarchical clustering structure.

**(3) Compute Cophenetic Distances**: For each pair of data points, calculate the cophenetic distance, which is the height at which the two points are first merged into the same cluster in the dendrogram.

**(4) Compute Cophenetic Correlation**: Once you have the original pairwise distances and the corresponding cophenetic distances, calculate the correlation coefficient between these two sets of distances. This correlation coefficient is the cophenetic correlation.

Mathematically, the cophenetic correlation $c$ is computed using the following formula:

$$c = \frac{\sum_{i < j}(d_{ij} - \bar{d})(c_{ij} - \bar{c})}{\sqrt{\sum_{i < j}(d_{ij} - \bar{d})^2 \sum_{i < j}(c_{ij} - \bar{c})^2}}$$

Where:

-   $d_{ij}$ is the original pairwise distance between data points $i$ and $j$.

-   $c_{ij}$ is the cophenetic distance between data points $i$ and $j$ in the dendrogram.

-   $\bar{d}$ is the mean of the original pairwise distances.

-   $\bar{c}$ is the mean of the cophenetic distances.

-   The summations are over all pairs of data points $i$ and $j$ with $i$ \< $j$.

(5) **Interpretation**: The resulting cophenetic correlation coefficient $c$ ranges from -1 to 1, where a value close to 1 indicates high similarity between the original pairwise distances and the distances represented by the dendrogram.

### **2. Silhouette Coefficient**

The Silhouette Coefficient is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). Silhouette Coefficient measures the relative distance between a data point and its own cluster versus neighboring clusters, providing insight into the quality and separation of clusters in a dataset.

It ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.

How to Compute Silhouette Coefficient?

(1) Compute the average distance from a single data point $i$ to all other points in the same cluster. Let this be denoted as $a_i$​.

(2) For each neighboring cluster (clusters other than the cluster $i$ belongs to), compute the average distance from $i$ to all points in that cluster. Let the minimum of these distances be denoted as $b_i$​.

(3) Compute the silhouette coefficient $s_i$​ for data point $i$ using the formula:

$$
s_i=\frac{b_i - a_i}{max(a_i, b_i)}
$$

(4) Repeat steps (1)-(3) for all data points.

(5) Finally, compute the overall Silhouette Coefficient as the mean of all $s_i$​ values.

$s_i​$ represents the Silhouette Coefficient for data point $i$, $a_i$​ represents the average distance from $i$ to all other points in the same cluster, and $b_i$​ represents the minimum average distance from $i$ to all points in neighboring clusters.

# Strengths, Weaknesses

### Strengths:

1.  **Interpretability**: Divisive clustering produces a hierarchical structure of clusters, making it easier to interpret and understand the relationships between clusters at different levels of granularity. This hierarchical representation can provide insights into the natural grouping of data and the hierarchical organization of clusters.

2.  **Flexibility**: Divisive clustering allows for the exploration of clusters at multiple levels of detail. Analysts can start with a single cluster encompassing all data points and recursively divide it into smaller clusters until reaching the desired level of granularity. This flexibility allows for adaptive clustering based on the characteristics of the data and the analytical objectives.

3.  **Scalability**: Divisive clustering can be more computationally efficient than agglomerative clustering, especially for large datasets. Since divisive clustering starts with a single cluster and recursively divides it into smaller clusters, it can be less computationally intensive than agglomerative clustering, which involves pairwise comparisons and merging of clusters.

4.  **Hierarchy Preservation**: Divisive clustering preserves the hierarchical relationships between clusters throughout the clustering process. This hierarchical structure provides additional context and information about the relationships between clusters, which can be useful for downstream analysis tasks such as visualization, interpretation, and pattern discovery.

5.  **Top-Down Exploration**: Divisive clustering allows analysts to perform top-down exploration of the data, starting with a global view of the entire dataset and progressively refining the clustering structure to reveal finer-grained patterns and relationships. This top-down approach can be beneficial for gaining insights into the hierarchical organization of data and identifying clusters at different levels of abstraction.

### Weaknesses:

1.  **Sensitivity to Initial Conditions**: Divisive clustering heavily relies on the initial selection of clusters, which can affect the final clustering results. Depending on the choice of initial cluster seeds, the algorithm may converge to different solutions, leading to variations in the clustering outcome.

2.  **Computational Complexity**: Divisive clustering can be computationally intensive, especially for large datasets. Since it involves recursively dividing clusters into smaller clusters, the algorithm's complexity increases with the number of data points and the depth of the clustering hierarchy. This can make divisive clustering less scalable for very large datasets compared to other clustering methods.

3.  **Over-Splitting**: Divisive clustering may suffer from over-splitting, where the algorithm continues to divide clusters until each data point forms its own cluster. This can result in excessively fine-grained clusters that may not capture meaningful patterns or relationships in the data. Over-splitting can occur if the algorithm's stopping criterion is not well-defined or if the data lacks clear natural clustering structure.

4.  **Difficulty in Determining the Number of Clusters**: Unlike some other clustering methods that produce a fixed number of clusters as output, divisive clustering generates a hierarchy of clusters, making it challenging to determine the optimal number of clusters. Analysts may need to rely on heuristic approaches or domain knowledge to interpret the clustering hierarchy and select an appropriate level of granularity.

5.  **Interpretability of Deep Hierarchies**: Divisive clustering can produce deep hierarchical structures with multiple levels of nesting, which may be difficult to interpret and visualize. Deep hierarchies may obscure meaningful patterns in the data and make it challenging to extract actionable insights from the clustering results.

6.  **Dependency on Distance Metric**: The choice of distance metric used in divisive clustering can significantly impact the clustering results. Different distance metrics may lead to different clustering outcomes, making it important to carefully select an appropriate distance measure based on the characteristics of the data and the clustering objectives.

# Applications:

Divisive hierarchical clustering (top-down clustering) has several uses across various fields due to its interpretability, flexibility, and ability to reveal hierarchical relationships within data. Some common applications of divisive hierarchical clustering include:

1.  **Exploratory Data Analysis**: Divisive clustering can be used for exploratory analysis to uncover hidden patterns, structures, and relationships within datasets. By recursively dividing clusters into smaller sub-clusters, analysts can gain insights into the hierarchical organization of data and identify meaningful groupings of data points.

2.  **Taxonomy Construction**: Divisive clustering is commonly used in taxonomy construction to classify organisms, species, or other entities into hierarchical categories. By clustering data based on similarities in attributes or characteristics, divisive clustering can help construct taxonomic hierarchies that reflect the evolutionary relationships between different groups.

3.  **Market Segmentation**: Divisive clustering is employed in market segmentation to identify distinct segments or subgroups of customers based on their purchasing behavior, preferences, demographics, or other characteristics. By partitioning the customer base into hierarchical segments, businesses can tailor marketing strategies and product offerings to better meet the needs of different customer segments.

4.  **Image Segmentation**: Divisive clustering is used in image processing and computer vision to segment images into meaningful regions or objects. By recursively dividing image pixels into clusters based on color, texture, or intensity similarities, divisive clustering can help separate foreground objects from background and segment complex scenes into semantically meaningful regions.

5.  **Biological Data Analysis**: Divisive clustering is applied in biological data analysis to classify gene expression patterns, protein sequences, or other biological data into hierarchical clusters. By grouping genes or proteins based on similarities in expression profiles or sequences, divisive clustering can help identify functional relationships and regulatory mechanisms within biological systems.

6.  **Document Clustering**: Divisive clustering is used in text mining and natural language processing to cluster documents or text data into hierarchical categories. By partitioning documents based on similarities in content or topics, divisive clustering can help organize large document collections, facilitate information retrieval, and support document summarization and categorization tasks.

# References

Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Second Edition. Springer, 2009.

mitran-lab.amath.nuc.edu/courses/MATH590/biblio/Clustering.ch7.HierarchicalClustering.pdf, SIAM Online Journals
<<<<<<< HEAD

Practical Guide to Cluster Analysis in R , ch9, page 84-96, by A. Kassambara

Everitt, B. S., Landau, S., Leese, M., & Stahl, D. (2011). Hierarchical clustering. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 1(1), 14-23.

Murtagh, F. (2011). On ultrametricity, data coding, and computation. Journal of Classification, 28(3), 326-347.

R Development Core Team. (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing.

Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20, 53-65.

Tibshirani, R., Walther, G., & Hastie, T. (2001). Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), 411-423.

# RShiny App
=======
>>>>>>> 0d067522c594f10d0e00f2b4c27164c55686e106



# RShiny App
```{r}
library(shiny)
library(shinydashboard)
library(stats)

ui <- dashboardPage(
  dashboardHeader(title = "Dendrogram Viewer for Studio Ghibli Dataset"),
  dashboardSidebar(
    fileInput("dataFile", "Upload your Studio Ghibli dataset", accept = ".csv"),
    selectInput("numColumns", "Select numeric columns for clustering",
                choices = NULL, selected = NULL, multiple = TRUE),
    selectInput("distanceType", "Select Distance Type",
                choices = c("euclidean", "maximum", "manhattan", "canberra", "binary", "minkowski"), selected = "euclidean"),
    selectInput("linkageMethod", "Select Linkage Method",
                choices = c("ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid"), selected = "complete"),
    actionButton("update", "Update Selection")
  ),
  dashboardBody(
    plotOutput("dendrogramPlot"),
    verbatimTextOutput("fileInfo")
  )
)

server <- function(input, output, session) {
  data <- reactive({
    inFile <- input$dataFile
    if (is.null(inFile)) {
      return(NULL)
    }
    read.csv(inFile$datapath, na.strings = c("NA", "na", ""))
  })
  
  observe({
    df <- data()
    updateSelectInput(session, "numColumns", choices = names(df)[sapply(df, is.numeric)])
  })

  output$fileInfo <- renderPrint({
    df <- data()
    if (is.null(df)) {
      "No file uploaded."
    } else {
      summary(df)
    }
  })

  output$dendrogramPlot <- renderPlot({
    req(input$update)
    df <- data()
    if (is.null(df) || is.null(input$numColumns)) {
      return()
    }
    clean_data <- df[, input$numColumns]
    clean_data <- apply(clean_data, 2, function(column) {
      if (any(is.na(column))) {
        column[is.na(column)] <- mean(column, na.rm = TRUE)
      }
      return(column)
    })

    dist_matrix <- tryCatch({
      dist(clean_data, method = input$distanceType)
    }, warning = function(w) {
      cat("Warning: ", w$message, "\n")
      return(NULL)
    }, error = function(e) {
      cat("Error: ", e$message, "\n")
      return(NULL)
    })

    if (!is.null(dist_matrix)) {
      hc <- hclust(dist_matrix, method = input$linkageMethod)
      plot(hc, main = "Dendrogram of Hierarchical Clustering")
    }
  })
}

shinyApp(ui, server)
```

```{r}
library(dendextend)
library(ggplot2)
library(plotly)
library(shiny)
library(factoextra)
library(cluster)
library(stats)

data = read.csv("Studio Ghibli [TEST].csv", header = TRUE, row.names = 1)

data_new <- na.omit(data)
data_new <- data_new[c("Year", "Budget", "Revenue", "Duration")]
col_names <- c("Year", "Budget_in_$M", "Revenue_in_$", "Duration_in_Minutes")
data_new <- setNames(data_new, col_names)
# data_new <- matrix(data_new)

data_new

clean_data <- apply(data_new, 2, function(column) {
  if(any(is.na(column))) {
    column[is.na(column)] <- mean(column, na.rm = TRUE)
  }
  return(column)
})

dend <- data_new %>%
  dist() %>%
  hclust(method = "ave") %>%
  as.dendrogram()
dend2 <- color_branches(dend, 5)

p <- ggplot(dend2, horiz = T, offset_labels = -3)
ggplotly(p)



# data_new
# write.csv(data_new, "Studio Ghiblie [TEST2].csv")
```
